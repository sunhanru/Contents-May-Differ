{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages \n",
    "\n",
    "distance() function allows you to compute the distance of any two Cartesian points in the 2D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def distance(p0, p1):\n",
    "    return math.sqrt((p0[0] - p1[0])**2 + (p0[1] - p1[1])**2)\n",
    "\n",
    "#import more packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data, stored as .txt files in the dataset/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this PREPROCESS, whenever Concorde chooses to branch on an edge with a zero LP value, we omit that data point\n",
    "\n",
    "#path of the directory containing data\n",
    "path = \"./dataset/\"\n",
    "dirs = os.listdir(path)\n",
    "\n",
    "#number of Files in the dataset/ directory\n",
    "numFiles = 5696 #had to remove some data that had I/O issues\n",
    "#initial number of columns of my np matrix containing the data\n",
    "numCols = 50\n",
    "\n",
    "#TESTING: Smaller Dataset\n",
    "#numFiles = 100\n",
    "\n",
    "#instantiate matrices to hold preprocessed data\n",
    "Xall = np.zeros(shape=(numFiles, numCols))\n",
    "tempvec = np.zeros(shape=(numFiles, numCols))\n",
    "Yall = np.zeros(shape=(numFiles, 1))\n",
    "\n",
    "\n",
    "index = 0\n",
    "for file in dirs[:numFiles]:\n",
    "    fname = \"./dataset/\" + file\n",
    "    #get (X,Y) coordinates of each city of the TSP\n",
    "    coords = np.genfromtxt(fname=fname, skip_header=6,max_rows=50,\n",
    "                       usecols=(1,2))\n",
    "    \n",
    "    internal = coords[0,:]\n",
    "    \n",
    "    #Get the LP value of nonzero edges \n",
    "    data = np.genfromtxt(fname=fname, skip_header=58,usecols=(0,1,2))\n",
    "    X = np.zeros(shape=data[:,2].shape)\n",
    "    Y = np.zeros(shape=data[:,2].shape)\n",
    "    #Target is the edge Concorde decided to branch on\n",
    "    #Edge is represented as a (fr, to) tuple, where fr is the outgoing \n",
    "    #city and to is the receiving city\n",
    "    target = np.genfromtxt(fname=fname, skip_header=56,\n",
    "                           max_rows=1,usecols=(0,1))\n",
    "    target = [int(x) for x in target]\n",
    "\n",
    "    #Y contains all 0s except for that edge selected by Concorde to branch on.\n",
    "    #that one singular edge is set to 1\n",
    "    #X contains all edges distances \n",
    "    for i, num in enumerate(X):\n",
    "        fr = (coords[int(data[i,0]), 0]-internal[0], coords[int(data[i,0]), 1]-internal[1])\n",
    "        to = (coords[int(data[i,1]), 0]-internal[0], coords[int(data[i,1]), 1]-internal[1])\n",
    "        X[i] = distance(fr, to)\n",
    "        if target[0] == data[i, 0] and target[1] == data[i, 1]:\n",
    "            Y[i] = 1\n",
    "        elif target[1] == data[i, 0] and target[0] == data[i, 1]:\n",
    "            Y[i] = 1\n",
    "    \n",
    "    #if np.count_nonzero(Y) == 0:\n",
    "    #    print(\"DEBUG\", index)\n",
    "    \n",
    "    #The following two if statements take care of zero-padding\n",
    "    #so that all vectors are of the same size\n",
    "    if X.shape[0] > numCols:\n",
    "        z = np.zeros((numFiles, X.shape[0] - numCols))\n",
    "        Xall = np.concatenate((Xall,z), axis=1)\n",
    "        tempvec = np.concatenate((tempvec,z), axis=1)\n",
    "        numCols = X.shape[0]\n",
    "    \n",
    "    if X.shape[0] < numCols:\n",
    "        z = np.zeros((1, numCols - X.shape[0]))\n",
    "        z=z[0]\n",
    "        X = np.concatenate((X, z))\n",
    "        Y = np.concatenate((Y, z))\n",
    "        \n",
    "    #if np.count_nonzero(Y) == 0:\n",
    "    #    print(\"DEBUG\", index)\n",
    "    \n",
    "    Xall[index,:] = X\n",
    "    tempvec[index,:] = Y\n",
    "      \n",
    "    index = index + 1\n",
    "\n",
    "#Filter out matrices with all zeros; in those instances, Concorde chose to branch on an edge with LP value of 0\n",
    "Xall = Xall[~np.all(tempvec == 0, axis=1)]\n",
    "tempvec = tempvec[~np.all(tempvec == 0, axis=1)]\n",
    "\n",
    "Yall = np.argmax(tempvec, axis=1)\n",
    "X = Xall\n",
    "y = Yall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Neural_Net(nn.Module):\n",
    "    def __init__(self, X):\n",
    "        super(My_Neural_Net, self).__init__()\n",
    "        \n",
    "        X_dim = X.shape[1]\n",
    "        \n",
    "        \n",
    "        # Define the layers. This matches the image above \n",
    "        # Except that our input size is 1000 dimensions\n",
    "        self.layer_1 = nn.Linear(X_dim, 500)\n",
    "        self.layer_2 = nn.Linear(500, 500)\n",
    "        self.layer_3 = nn.Linear(500, X_dim) \n",
    "        \n",
    "        # Define activation functions.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        #Choose Adam as the optimization method\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "#1. input X\n",
    "    def forward(self, X):\n",
    "        # 2. linearly transform X into hidden data 1 via weights\n",
    "        X = self.layer_1(X)\n",
    "        # 3. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 4. linearly transform hidden data into hidden data 2 via weights\n",
    "        X = self.layer_2(X)\n",
    "        # 5. perform ReLU on hidden data\n",
    "        X = self.relu(X)\n",
    "        # 6. linearly transform hidden data into output layer via weights\n",
    "        X = self.layer_3(X)\n",
    "        # 7. perform sigmoid on output data to get f(X) predictions between 0 and 1\n",
    "        X = self.softmax(X)\n",
    "        \n",
    "        # 8. output predictions\n",
    "        return X\n",
    "    \n",
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = nn.CrossEntropyLoss()\n",
    "        return score(pred, true)\n",
    "    \n",
    "    # 1. input: N - number of iterations to train, X - data, y - target\n",
    "    def fit(self,X,y, X_test, y_test, N = 1000):\n",
    "        \n",
    "        #first column is epoch/second column is loss\n",
    "        #third column is accuracy\n",
    "        #fourth column is loss for test\n",
    "        #fifth column is loss for test\n",
    "        losses = np.zeros(shape=(N,5))\n",
    "        \n",
    "        # 2. for n going from 0 to N -1 :\n",
    "        for epoch in range(N):\n",
    "                      \n",
    "            # Reset weights in case they are set for some reason\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # 3. f(X) = forward(X) \n",
    "            pred = self.forward(X)\n",
    "            \n",
    "            # 4. l = loss(f(X),y)\n",
    "            l = self.loss(pred, y)\n",
    "            acc = (torch.argmax(pred, dim=1) == y).float().mean()\n",
    "            \n",
    "            pred_test = self.forward(X_test)\n",
    "            l_test = self.loss(pred_test, y_test)\n",
    "            acc_test = (torch.argmax(pred_test, dim=1) == y_test).float().mean()\n",
    "            \n",
    "            #print loss\n",
    "            #print(l.data)\n",
    "            losses[epoch, 0] = epoch\n",
    "            losses[epoch, 1] = l.data\n",
    "            losses[epoch, 2] = acc\n",
    "            losses[epoch, 3] = l_test.data\n",
    "            losses[epoch, 4] = acc_test\n",
    "            \n",
    "            # 5. Back progation\n",
    "            l.backward()\n",
    "            # 5. Gradient Descent\n",
    "            self.optimizer.step()\n",
    "        \n",
    "        return losses\n",
    "        \n",
    "    #From a list of a \"metric\" encoded for each edge (e.g., the distance multiplied by LP value)\n",
    "    #predict_distribution gives the likelihood of each edge being the successful branching node\n",
    "    def predict_distribution(self, X):\n",
    "        distributions = self.forward(X)\n",
    "                \n",
    "        return distributions\n",
    "    \n",
    "    #This returns the index of the most probable successful branching node from the list of edges\n",
    "    def predict(self, X):\n",
    "        probs = self.predict_distribution(X)\n",
    "        \n",
    "        return torch.argmax(probs, dim=1)\n",
    "           \n",
    "    #Checks how often the most probable successful branching node predicted by DNN is the same\n",
    "    #as the branching decision made by Concorde TSP Solver\n",
    "    def score(self, X, y):\n",
    "        # proportion of times where we're correct\n",
    "        acc = (self.predict(X) == y).float().mean()\n",
    "        \n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset into train and test and train on DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0e567327e5a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mneur_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMy_Neural_Net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mneur_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_tens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_tens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-29fbf5a99834>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, X_test, y_test, N)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;31m# 5. Back progation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;31m# 5. Gradient Descent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Split into train and test so we can fit on some data and see performance \n",
    "# on data we havent seen yet.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Turn X and y (train and test) into PyTorch objects. We always have to do this step\n",
    "X_train_tens = Variable(torch.Tensor(X_train).float())\n",
    "X_test_tens = Variable(torch.Tensor(X_test).float())\n",
    "y_train_tens = Variable(torch.Tensor(y_train).long())\n",
    "y_test_tens = Variable(torch.Tensor(y_test).long())\n",
    "\n",
    "neur_net = My_Neural_Net(X_train_tens)\n",
    "losses = neur_net.fit(X_train_tens, y_train_tens,X_test_tens, y_test_tens, N=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training results and  get final test/train accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two subplots, the axes array is 1-d\n",
    "f, axarr = plt.subplots(2, sharex=True, figsize=(8,8))\n",
    "axarr[0].plot(losses[:,0], losses[:,1], label='Train')\n",
    "axarr[0].plot(losses[:,0], losses[:,3], label='Test')\n",
    "axarr[0].set_ylabel(\"Cross Entropy Loss\")\n",
    "axarr[0].legend(loc=\"upper right\")\n",
    "axarr[0].set_title(\"DNN Trainining: Distance Only\")\n",
    "axarr[1].plot(losses[:,0], losses[:,2],label='Train')\n",
    "axarr[1].plot(losses[:,0], losses[:,4], label='Test')\n",
    "axarr[1].set_ylabel(\"Accuracy\")\n",
    "axarr[1].set_xlabel(\"Epoch\")\n",
    "axarr[1].legend(loc=\"lower right\")\n",
    "\n",
    "print(\"score for test data: \", neur_net.score(X_test_tens,y_test_tens))\n",
    "print(\"score for train data: \", neur_net.score(X_train_tens,y_train_tens))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"D.csv\", losses, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
